# API Reference

This section provides detailed API documentation for vLLM's components.

## Core Components

- [LLM Engine](engine.md): The core engine for model inference
- [Sampling Parameters](sampling_params.md): Configuration for text generation
- [Worker](worker.md): Worker process management

## Features

- [LoRA Support](lora.md): Low-Rank Adaptation support
- [Storage and Cache](storage.md): Adapter storage and caching system
- [Quantization](quantization.md): Model quantization support
- [Tensor Parallelism](tensor_parallel.md): Distributed inference

## Server Components

- [FastAPI Server](server.md): REST API server implementation
- [AsyncEngine](async_engine.md): Asynchronous inference engine
- [OpenAI API](openai.md): OpenAI-compatible API endpoints 