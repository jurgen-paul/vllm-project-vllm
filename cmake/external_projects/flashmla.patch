diff --git a/csrc/flash_api.cpp b/csrc/flash_api.cpp
index 5a1cb8e..65fbfb0 100644
--- a/csrc/flash_api.cpp
+++ b/csrc/flash_api.cpp
@@ -1,6 +1,6 @@
 // Adapted from https://github.com/Dao-AILab/flash-attention/blob/main/csrc/flash_attn/flash_api.cpp
 
-#include <torch/python.h>
+// #include <torch/python.h>
 #include <torch/nn/functional.h>
 #include <ATen/cuda/CUDAContext.h>
 #include <c10/cuda/CUDAGuard.h>
@@ -196,8 +196,14 @@ mha_fwd_kvcache_mla(
     return {out, softmax_lse};
 }
 
-PYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {
-    m.doc() = "FlashMLA";
-    m.def("get_mla_metadata", &get_mla_metadata);
-    m.def("fwd_kvcache_mla", &mha_fwd_kvcache_mla);
+#include "core/registration.h"
+#include "pytorch_shim.h"
+
+TORCH_LIBRARY_EXPAND(TORCH_EXTENSION_NAME, m) {
+    m.def("get_mla_metadata", make_pytorch_shim(&get_mla_metadata));
+    m.impl("get_mla_metadata", torch::kCUDA, make_pytorch_shim(&get_mla_metadata));
+
+    m.def("fwd_kvcache_mla", make_pytorch_shim(&mha_fwd_kvcache_mla));
+    m.impl("fwd_kvcache_mla", torch::kCUDA, make_pytorch_shim(&mha_fwd_kvcache_mla));
 }
+REGISTER_EXTENSION(TORCH_EXTENSION_NAME)
\ No newline at end of file
